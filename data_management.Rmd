---
title: "Data Management"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)

# https://readxl.tidyverse.org/
library(tidyverse)
library(readxl)
library(DT)
```

---

# Data Handling

## Summarization

1.  Average across lab replicates<br>
2.  Average across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)<br>
3.  Average across multiple lab samples (if they exist) for the same sample number (split) and lab.  Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.<br>
4.  Average across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample  number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.<br>
5.  Average across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.

---

# Chemistry

```{r, include=F}
wb <- "./static/Chemical_Lists.xls"

chem_lists <- read_excel(wb, sheet=1)
cwa <- read_excel(wb, sheet=2)
pfas <- read_excel(wb, sheet=3)
pesticides <- read_excel(wb, sheet=4)
pcb <- read_excel(wb, sheet=5)
pah <- read_excel(wb, sheet=6)
fertilizers <- read_excel(wb, sheet=7)
dioxfuran <- read_excel(wb, sheet=8)
pbde <- read_excel(wb, sheet=9)
```

## Dioxin & Furans
```{r}
datatable(dioxfuran, options=list(scrollX=T))
```

---

## PAHs
```{r}
datatable(pah, options=list(scrollX=T))
```

---

## PCBs
```{r}
datatable(pcb, options=list(scrollX=T))
```

---

## PFAS
```{r}
datatable(pfas, options=list(scrollX=T))
```

--- 

## Source
[Chemical Lists](./static/Chemical_Lists.xls)

---

# Character Encoding

We deal with character encoding issues when importing data to databases all the time.  All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft's custom encoding, which is CP-1252 (also known as win-1252 and a few other things).  The character encoding of a file can't necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job.  (It's available on the "rstudio" server.)  Also, the Geany editor does a good job of diagnosing file formats.  If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you.  There's a Python library on PyPI named chardet that will also diagnose file encodings.

For data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252.  That covers about 90% of cases.  Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases.  For the remainders, Geany is usually the quickest way to check the file encoding.

Everything that comes out of our databases is always in UTF-8, so I, at least, don't ordinarily have encoding issues when importing data to R.  For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools.

---

# CMD

## Syntax
[Source](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/cmd)

**Syntax**: `cmd [/c|/k] [/s] [/q] [/d] [/a|/u] [/t:{<b><f> | <f>}] [/e:{on | off}] [/f:{on | off}] [/v:{on | off}] [<string>]`

**Multiple commands**: `"<command1>&&<command2>&&<command3>"`

## Parameters
```{r cmd, echo=F}
# library(tidyverse)
# library(readxl)
library(DT)
f <- "./static/cmd.csv"
tbl <- read.csv(f)

# Show all rows in table (no pagination)
datatable(tbl, options=list(paging=FALSE))
```

